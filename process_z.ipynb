{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tabulate\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p1 = r\"(?P<misson_name>.+)_(?P<gamma>\\d+(\\.\\d+)?)_(?P<delta>.+)_(?P<threshold>.+)_z\"\n",
    "## \n",
    "## file = \"alpacafarm_0.75_5.0_6.0_z.jsonl\"\n",
    "## matcher1 = re.match(p1, file)\n",
    "## misson_name = matcher1.group(\"misson_name\")\n",
    "## threshold = matcher1.group(\"threshold\")\n",
    "## \n",
    "## print(misson_name, threshold)\n",
    "\n",
    "# g0.5 d5.0\n",
    "# g0.25 d5.0\n",
    "# g0.75 d5.0\n",
    "# g0.5 d2.0\n",
    "# g0.25 d2.0\n",
    "# g0.9 d5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subfolder is: llama2-7b-chat-4k_gpt_g0.25_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k gpt 0.25 10.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.1_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k gpt 0.1 10.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d15.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.25 15.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.75_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.75 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.25_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k v2 0.25 2.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.5_d2.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.5 2.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.25 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.5 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.1_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.1 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.25_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k v2 0.25 10.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.1_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.1 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d10.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.25 10.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.9_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.9 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.5_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k old 0.5 2.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.5_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k gpt 0.5 2.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.9_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.9 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k old 0.25 2.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.9_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.9 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.1_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.1 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.5 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.25_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.25 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.25_d15.0\n",
      "d15\n",
      "llama2-7b-chat-4k gpt 0.25 15.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.1_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k v2 0.1 10.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.75_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.75 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.5_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k v2 0.5 2.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d15.0\n",
      "d15\n",
      "llama2-7b-chat-4k old 0.25 15.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.9_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.9 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.5_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.5 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_new_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k new 0.5 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_no_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k no 0.5 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.75_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.75 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.1_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.1 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d2.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.25 2.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.25_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.25 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.75_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.75 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.1_d10.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.1 10.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.5 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.25 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.25_d15.0\n",
      "d15\n",
      "llama2-7b-chat-4k v2 0.25 15.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k old 0.25 10.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.1_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k old 0.1 10.0 soft\n",
      "            model_name            mission_name mode gamma delta  threshold  \\\n",
      "0    llama2-7b-chat-4k                   qmsum  gpt  0.25  10.0        6.0   \n",
      "1    llama2-7b-chat-4k              alpacafarm  gpt  0.25  10.0        6.0   \n",
      "2    llama2-7b-chat-4k                hotpotqa  gpt  0.25  10.0        6.0   \n",
      "3    llama2-7b-chat-4k             longform_qa  gpt  0.25  10.0        6.0   \n",
      "4    llama2-7b-chat-4k                     lcc  gpt  0.25  10.0        6.0   \n",
      "..                 ...                     ...  ...   ...   ...        ...   \n",
      "358  llama2-7b-chat-4k                hotpotqa  old   0.1  10.0        6.0   \n",
      "359  llama2-7b-chat-4k  konwledge_memorization  old   0.1  10.0        6.0   \n",
      "360  llama2-7b-chat-4k              multi_news  old   0.1  10.0        6.0   \n",
      "361  llama2-7b-chat-4k              finance_qa  old   0.1  10.0        6.0   \n",
      "362  llama2-7b-chat-4k             longform_qa  old   0.1  10.0        6.0   \n",
      "\n",
      "    bl_type    z_score  sum  \n",
      "0      None  18.372595  200  \n",
      "1      None  24.303930  805  \n",
      "2      None   2.740259  200  \n",
      "3      None  28.158617  200  \n",
      "4      None   8.304532  198  \n",
      "..      ...        ...  ...  \n",
      "358    soft   6.149754  200  \n",
      "359    soft  10.674784  200  \n",
      "360    soft  63.581417  200  \n",
      "361    soft  48.024014  200  \n",
      "362    soft  49.479527  200  \n",
      "\n",
      "[363 rows x 9 columns]\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "# load data from pred\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=[\"model_name\", \"mission_category\", \"mode\", \"gamma\", \"delta\", \"threshold\", \"bl_type\", \"z_score\", \"sum\"])\n",
    "\n",
    "input_dir = \"./pred\"\n",
    "p = r\"(?P<model_name>.+)_(?P<mode>old|v2|gpt|new|no)_g(?P<gamma>.+)_d(?P<delta>\\d+(\\.\\d+)?)\"\n",
    "p1 = r\"(?P<misson_name>[a-zA-Z_]+)_(?P<gamma>\\d+(\\.\\d+)?)_(?P<delta>.+)_z\"\n",
    "\n",
    "num = 0\n",
    "# get all files from input_dir\n",
    "for subfolder in os.listdir(input_dir):\n",
    "    print(\"subfolder is:\", subfolder)\n",
    "    matcher = re.match(p, subfolder)\n",
    "    model_name = matcher.group(\"model_name\")\n",
    "    mode = matcher.group(\"mode\")\n",
    "    gamma = matcher.group(\"gamma\")\n",
    "    delta = matcher.group(\"delta\")\n",
    "    \n",
    "    bl_type = \"None\"\n",
    "    bl_type = (subfolder.split(\"_\")[-1]).split(\".\")[0]\n",
    "    \n",
    "    print(bl_type)\n",
    "    if bl_type != \"hard\":\n",
    "        if \"old\" in subfolder:\n",
    "            bl_type = \"soft\"\n",
    "        else:\n",
    "            bl_type = \"None\"\n",
    "        \n",
    "        \n",
    "    print(model_name, mode, gamma, delta, bl_type)\n",
    "    \n",
    "    z_score_path = os.path.join(input_dir, subfolder, \"z_score\")\n",
    "    if os.path.exists(z_score_path):\n",
    "        # print(\"subfolder is:\", subfolder)\n",
    "        files = os.listdir(z_score_path)\n",
    "        \n",
    "        for file in files:\n",
    "            # print(file)\n",
    "            # read jsons\n",
    "            matcher1 = re.match(p1, file)\n",
    "            if matcher1:\n",
    "                misson_name = matcher1.group(\"misson_name\")\n",
    "                threshold = 6.0\n",
    "            else:\n",
    "                threshold = file.split(\"_\")[-2]\n",
    "            \n",
    "            with open(os.path.join(z_score_path, file), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # get data\n",
    "            avarage_z = data[\"avarage_z\"]\n",
    "            sum = len(data[\"z_score_list\"])\n",
    "            # wm_pred_avarage = data[\"wm_pred_avarage\"]\n",
    "            \n",
    "            temp = pd.DataFrame({\n",
    "                \"model_name\": [model_name],\n",
    "                \"mode\": [mode],\n",
    "                \"mission_category\": [misson_name],\n",
    "                \"gamma\": [gamma],\n",
    "                \"delta\":[delta],\n",
    "                \"threshold\": [threshold],\n",
    "                \"bl_type\": [bl_type],\n",
    "                \"z_score\": [avarage_z],\n",
    "                \"sum\": [sum]})\n",
    "            \n",
    "            df = pd.concat([df, temp], ignore_index=True)\n",
    "            num += 1\n",
    "\n",
    "df.to_csv(\"z_score.csv\")           \n",
    "print(df)\n",
    "print(num)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            model_name             mission_name mode gamma delta  threshold  \\\n",
      "255  llama2-7b-chat-4k                      lcc   no   0.5   5.0        6.0   \n",
      "238  llama2-7b-chat-4k                 hotpotqa  old   0.9   5.0        6.0   \n",
      "242  llama2-7b-chat-4k  konwledge_understanding  old   0.9   5.0        6.0   \n",
      "218  llama2-7b-chat-4k  konwledge_understanding   v2   0.5   2.0        6.0   \n",
      "161  llama2-7b-chat-4k  konwledge_understanding  old   0.9   5.0        6.0   \n",
      "..                 ...                      ...  ...   ...   ...        ...   \n",
      "360  llama2-7b-chat-4k               multi_news  old   0.1  10.0        6.0   \n",
      "12   llama2-7b-chat-4k                    qmsum  gpt   0.1  10.0        6.0   \n",
      "276  llama2-7b-chat-4k               multi_news  old   0.1   5.0        6.0   \n",
      "315  llama2-7b-chat-4k               multi_news  old   0.1  10.0        6.0   \n",
      "15   llama2-7b-chat-4k               multi_news  gpt   0.1  10.0        6.0   \n",
      "\n",
      "    bl_type    z_score  sum  \n",
      "255    None  -0.915615  200  \n",
      "238    soft  -0.717057  200  \n",
      "242    soft  -0.534699  200  \n",
      "218    None  -0.507295  200  \n",
      "161    hard  -0.487937  200  \n",
      "..      ...        ...  ...  \n",
      "360    soft  63.581417  200  \n",
      "12     None  65.750604  200  \n",
      "276    hard  66.190720  200  \n",
      "315    hard  66.190720  200  \n",
      "15     None  66.471030  200  \n",
      "\n",
      "[363 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort_values(by=\"z_score\", ascending=True)\n",
    "df_sorted.to_csv(\"z_score_sorted.csv\")           \n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subfolder is: ref_old_soft_g0.1_d10.0\n",
      "old_soft_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_hard_g0.25_d5.0\n",
      "old_soft_g0.25_d15.0\n",
      "v2_g0.25_d15.0\n",
      "subfolder is: ref_old_hard_g0.25_d5.0\n",
      "old_hard_g0.25_d5.0\n",
      "old_soft_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_soft_g0.25_d15.0\n",
      "v2_g0.25_d15.0\n",
      "subfolder is: ref_old_soft_g0.25_d15.0\n",
      "old_soft_g0.25_d15.0\n",
      "old_soft_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_hard_g0.25_d5.0\n",
      "v2_g0.25_d15.0\n",
      "subfolder is: ref_gpt_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_soft_g0.1_d10.0\n",
      "old_hard_g0.25_d5.0\n",
      "old_soft_g0.25_d15.0\n",
      "v2_g0.25_d15.0\n",
      "subfolder is: ref_v2_g0.25_d15.0\n",
      "v2_g0.25_d15.0\n",
      "old_soft_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_hard_g0.25_d5.0\n",
      "old_soft_g0.25_d15.0\n",
      "            model_name             mission_name             ref_mode  \\\n",
      "0    llama2-7b-chat-4k               multi_news  old_soft_g0.1_d10.0   \n",
      "1    llama2-7b-chat-4k               alpacafarm  old_soft_g0.1_d10.0   \n",
      "2    llama2-7b-chat-4k                      lcc  old_soft_g0.1_d10.0   \n",
      "3    llama2-7b-chat-4k              longform_qa  old_soft_g0.1_d10.0   \n",
      "4    llama2-7b-chat-4k   konwledge_memorization  old_soft_g0.1_d10.0   \n",
      "..                 ...                      ...                  ...   \n",
      "175  llama2-7b-chat-4k   konwledge_memorization       v2_g0.25_d15.0   \n",
      "176  llama2-7b-chat-4k  konwledge_understanding       v2_g0.25_d15.0   \n",
      "177  llama2-7b-chat-4k                 hotpotqa       v2_g0.25_d15.0   \n",
      "178  llama2-7b-chat-4k               finance_qa       v2_g0.25_d15.0   \n",
      "179  llama2-7b-chat-4k                    qmsum       v2_g0.25_d15.0   \n",
      "\n",
      "                 det_mode threshold    z_score  sum  \n",
      "0          gpt_g0.1_d10.0       4.0  -2.051204  200  \n",
      "1          gpt_g0.1_d10.0       4.0  -1.545221  805  \n",
      "2          gpt_g0.1_d10.0       4.0  -0.608703  200  \n",
      "3          gpt_g0.1_d10.0       4.0  -1.884483  200  \n",
      "4          gpt_g0.1_d10.0       4.0   0.170509  200  \n",
      "..                    ...       ...        ...  ...  \n",
      "175  old_soft_g0.25_d15.0       4.0  -3.069688  200  \n",
      "176  old_soft_g0.25_d15.0       4.0  -1.915622  200  \n",
      "177  old_soft_g0.25_d15.0       4.0  -2.144303  200  \n",
      "178  old_soft_g0.25_d15.0       4.0  -9.799492  200  \n",
      "179  old_soft_g0.25_d15.0       4.0 -10.246890  200  \n",
      "\n",
      "[180 rows x 7 columns]\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "### process_mutual\n",
    "df = pd.DataFrame(columns=[\"model_name\", \"mission_name\", \"ref_mode\", \"det_mode\", \"threshold\", \"z_score\", \"sum\"])\n",
    "\n",
    "input_dir = \"./mutual_detect/llama2-7b-chat-4k\"\n",
    "\n",
    "num = 0\n",
    "p1 = r\"(?P<misson_name>[a-zA-Z_]+)_(?P<threshold>\\d+(\\.\\d+)?)_z\"\n",
    "\n",
    "# get all files from input_dir\n",
    "for subfolder in os.listdir(input_dir):\n",
    "    print(\"subfolder is:\", subfolder)\n",
    "    \n",
    "    ref_mode = subfolder.split(\"ref_\")[1]\n",
    "        \n",
    "    print(ref_mode)\n",
    "    \n",
    "    for subsubfolder in os.listdir(os.path.join(input_dir,subfolder)):\n",
    "        \n",
    "        det_mode = subsubfolder.split(\"_z\")[0]\n",
    "        print(det_mode)\n",
    "    \n",
    "        z_score_path = os.path.join(input_dir, subfolder, subsubfolder)\n",
    "        if os.path.exists(z_score_path):\n",
    "        # print(\"subfolder is:\", subfolder)\n",
    "            files = os.listdir(z_score_path)\n",
    "        \n",
    "            for file in files:\n",
    "                # print(file)\n",
    "                # read jsons\n",
    "                matcher1 = re.match(p1, file)\n",
    "                if matcher1:\n",
    "                    misson_name = matcher1.group(\"misson_name\")\n",
    "                    threshold = matcher1.group(\"threshold\")\n",
    "                with open(os.path.join(z_score_path, file), \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                # get data\n",
    "                avarage_z = data[\"avarage_z\"]\n",
    "                sum = len(data[\"z_score_list\"])\n",
    "                # wm_pred_avarage = data[\"wm_pred_avarage\"]\n",
    "                \n",
    "                temp = pd.DataFrame({\n",
    "                    \"model_name\": [model_name],\n",
    "                    \"ref_mode\": [ref_mode],\n",
    "                    \"mission_name\": [misson_name],\n",
    "                    \"det_mode\":[det_mode],\n",
    "                    \"threshold\": [threshold],\n",
    "                    \"z_score\": [avarage_z],\n",
    "                    \"sum\": [sum]})\n",
    "                \n",
    "                df = pd.concat([df, temp], ignore_index=True)\n",
    "                num += 1\n",
    "\n",
    "df.to_csv(\"mutual_z_score.csv\")           \n",
    "print(df)\n",
    "print(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size of chatglm2-6b-32k tokenizer is 64787\n",
      "The vocab size of chatglm2-6b-32k is 65024\n"
     ]
    }
   ],
   "source": [
    "# 导入transformers库\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "seed_everything(42)\n",
    "model2path = json.load(open(\"config/model2path.json\", \"r\"))\n",
    "dataset2maxlen = json.load(open(\"config/dataset2maxlen.json\", \"r\"))\n",
    "model_name = \"vicuna-13b\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 加载chatglm2-6b-32k模型的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model2path[model_name], trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model2path[model_name], trust_remote_code=True,\n",
    "                                                  output_scores=True, return_dict_in_generate=True, \n",
    "                                                  torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "# 获取tokenizer的词表大小\n",
    "vocab_size = model.config.padded_vocab_size\n",
    "vocab_size_tokenizer = len(tokenizer.get_vocab().values())\n",
    "\n",
    "# 打印词表大小\n",
    "print(\"The vocab size of chatglm2-6b-32k tokenizer is\", vocab_size_tokenizer)\n",
    "print(\"The vocab size of chatglm2-6b-32k is\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:13<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|User|>:You are a helpful assistant, please answer the following question within 300 words:\\n\\n( LI5 ) Explain to me the collapse of the Soviet Union . How did one of the largest superpowers manage to fall ? Is it possible something similar could eventually happen to the USA ? Explain like I'm five.<eoh>\\n<|Bot|>:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsq/anaconda3/envs/wmklmm/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'When', 'logprob': -1.390625}, {'text': ' K', 'logprob': -2.40625}, {'text': 'gb', 'logprob': -0.76953125}, {'text': ' officers', 'logprob': -0.126953125}, {'text': ' liquid', 'logprob': -5.0}, {'text': 'ations', 'logprob': -1.1015625}, {'text': ' schools', 'logprob': -4.625}, {'text': ' instead', 'logprob': -3.125}, {'text': ' by', 'logprob': -4.84375}, {'text': ' opening', 'logprob': -1.609375}, {'text': ' their', 'logprob': -0.6953125}, {'text': ' centers', 'logprob': -2.296875}, {'text': ' of', 'logprob': -1.390625}, {'text': ' advanced', 'logprob': -5.375}, {'text': ' ag', 'logprob': -4.1875}, {'text': 'itation', 'logprob': -1.140625}, {'text': ' and', 'logprob': -0.73828125}, {'text': ' infiltr', 'logprob': -1.390625}, {'text': 'ated', 'logprob': -0.69140625}, {'text': ' by', 'logprob': -0.6953125}, {'text': ' humanitarian', 'logprob': -6.8125}, {'text': ' funds', 'logprob': -5.53125}, {'text': ' with', 'logprob': -1.2734375}, {'text': ' NGO', 'logprob': -2.78125}, {'text': ' ', 'logprob': -3.328125}, {'text': '’', 'logprob': -1.359375}, {'text': ',', 'logprob': -4.84375}, {'text': ' which', 'logprob': -1.1015625}, {'text': ' took', 'logprob': -6.4375}, {'text': ' advantage', 'logprob': -0.6953125}, {'text': ' as', 'logprob': -3.265625}, {'text': ' leader', 'logprob': -6.3125}, {'text': ' to', 'logprob': -0.828125}, {'text': ' some', 'logprob': -5.125}, {'text': ' local', 'logprob': -2.203125}, {'text': ' officials', 'logprob': -0.0025177001953125}, {'text': ' people', 'logprob': -9.1875}, {'text': ' from', 'logprob': -1.609375}, {'text': ' mos', 'logprob': -4.15625}, {'text': '<eoa>', 'logprob': -4.0625}, {'text': '<0x0A>', 'logprob': 0.0}, {'text': '</s>', 'logprob': 0.0}]\n",
      "When Kgb officers liquidations schools instead by opening their centers of advanced agitation and infiltrated by humanitarian funds with NGO ’, which took advantage as leader to some local officials people from mos<eoa>\n",
      "\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "from generate import Generator\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "seed_everything(42)\n",
    "model_name = \"internlm-7b-8k\"\n",
    "model2path = json.load(open(\"config/model2path.json\", \"r\"))\n",
    "dataset2maxlen = json.load(open(\"config/dataset2maxlen.json\", \"r\"))\n",
    "dataset = \"alpacafarm\"\n",
    "max_gen = dataset2maxlen[dataset]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model2path[model_name], trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model2path[model_name], device_map='auto', return_dict_in_generate=True, output_scores=True,trust_remote_code=True\n",
    "        )\n",
    "\n",
    "watermark_args = Namespace(mode=\"v2\", gamma=0.25, delta=10.0, initial_seed=1234, dynamic_seed=None, bl_type=\"soft\", num_beams=1, sampling_temp=0.7, seeding_scheme=\"simple_1\", select_green_tokens=True)\n",
    "\n",
    "generator = Generator(watermark_args, tokenizer, model)\n",
    "with open(\"testfile.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data)\n",
    "\n",
    "# 对数据进行分词\n",
    "input = tokenizer(data, truncation=False, return_tensors=\"pt\").to(device)\n",
    "\n",
    "completions_text, completions_tokens  = generator.generate(input_ids=input.input_ids, max_new_tokens=max_gen)\n",
    "\n",
    "print(completions_tokens)\n",
    "print(completions_text)\n",
    "\n",
    "print(\"####################\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('wmklmm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acdb7ce7ce850a195ea1a66ef30c55071e5db04eb4f79a2da398a13b17183832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
