{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tabulate\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p1 = r\"(?P<misson_name>.+)_(?P<gamma>\\d+(\\.\\d+)?)_(?P<delta>.+)_(?P<threshold>.+)_z\"\n",
    "## \n",
    "## file = \"alpacafarm_0.75_5.0_6.0_z.jsonl\"\n",
    "## matcher1 = re.match(p1, file)\n",
    "## misson_name = matcher1.group(\"misson_name\")\n",
    "## threshold = matcher1.group(\"threshold\")\n",
    "## \n",
    "## print(misson_name, threshold)\n",
    "\n",
    "# g0.5 d5.0\n",
    "# g0.25 d5.0\n",
    "# g0.75 d5.0\n",
    "# g0.5 d2.0\n",
    "# g0.25 d2.0\n",
    "# g0.9 d5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subfolder is: llama2-7b-chat-4k_gpt_g0.25_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k gpt 0.25 10.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.1_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k gpt 0.1 10.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d15.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.25 15.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.75_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.75 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.25_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k v2 0.25 2.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.5_d2.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.5 2.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.25 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.5 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.1_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.1 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.25_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k v2 0.25 10.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.1_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.1 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d10.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.25 10.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.9_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.9 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.5_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k old 0.5 2.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.5_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k gpt 0.5 2.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.9_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.9 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k old 0.25 2.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.9_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.9 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.1_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.1 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.5 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.25_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.25 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.25_d15.0\n",
      "d15\n",
      "llama2-7b-chat-4k gpt 0.25 15.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.1_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k v2 0.1 10.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.75_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.75 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.5_d2.0\n",
      "d2\n",
      "llama2-7b-chat-4k v2 0.5 2.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d15.0\n",
      "d15\n",
      "llama2-7b-chat-4k old 0.25 15.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.9_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.9 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.5_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.5 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_new_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k new 0.5 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_no_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k no 0.5 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_gpt_g0.75_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k gpt 0.75 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.1_d5.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.1 5.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d2.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.25 2.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.25_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k v2 0.25 5.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.75_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.75 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.1_d10.0_hard\n",
      "hard\n",
      "llama2-7b-chat-4k old 0.1 10.0 hard\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.5_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.5 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d5.0\n",
      "d5\n",
      "llama2-7b-chat-4k old 0.25 5.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_v2_g0.25_d15.0\n",
      "d15\n",
      "llama2-7b-chat-4k v2 0.25 15.0 None\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.25_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k old 0.25 10.0 soft\n",
      "subfolder is: llama2-7b-chat-4k_old_g0.1_d10.0\n",
      "d10\n",
      "llama2-7b-chat-4k old 0.1 10.0 soft\n",
      "            model_name            mission_name mode gamma delta  threshold  \\\n",
      "0    llama2-7b-chat-4k                   qmsum  gpt  0.25  10.0        6.0   \n",
      "1    llama2-7b-chat-4k              alpacafarm  gpt  0.25  10.0        6.0   \n",
      "2    llama2-7b-chat-4k                hotpotqa  gpt  0.25  10.0        6.0   \n",
      "3    llama2-7b-chat-4k             longform_qa  gpt  0.25  10.0        6.0   \n",
      "4    llama2-7b-chat-4k                     lcc  gpt  0.25  10.0        6.0   \n",
      "..                 ...                     ...  ...   ...   ...        ...   \n",
      "358  llama2-7b-chat-4k                hotpotqa  old   0.1  10.0        6.0   \n",
      "359  llama2-7b-chat-4k  konwledge_memorization  old   0.1  10.0        6.0   \n",
      "360  llama2-7b-chat-4k              multi_news  old   0.1  10.0        6.0   \n",
      "361  llama2-7b-chat-4k              finance_qa  old   0.1  10.0        6.0   \n",
      "362  llama2-7b-chat-4k             longform_qa  old   0.1  10.0        6.0   \n",
      "\n",
      "    bl_type    z_score  sum  \n",
      "0      None  18.372595  200  \n",
      "1      None  24.303930  805  \n",
      "2      None   2.740259  200  \n",
      "3      None  28.158617  200  \n",
      "4      None   8.304532  198  \n",
      "..      ...        ...  ...  \n",
      "358    soft   6.149754  200  \n",
      "359    soft  10.674784  200  \n",
      "360    soft  63.581417  200  \n",
      "361    soft  48.024014  200  \n",
      "362    soft  49.479527  200  \n",
      "\n",
      "[363 rows x 9 columns]\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "# load data from pred\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=[\"model_name\", \"mission_category\", \"mode\", \"gamma\", \"delta\", \"threshold\", \"bl_type\", \"z_score\", \"sum\"])\n",
    "\n",
    "input_dir = \"./pred\"\n",
    "p = r\"(?P<model_name>.+)_(?P<mode>old|v2|gpt|new|no)_g(?P<gamma>.+)_d(?P<delta>\\d+(\\.\\d+)?)\"\n",
    "p1 = r\"(?P<misson_name>[a-zA-Z_]+)_(?P<gamma>\\d+(\\.\\d+)?)_(?P<delta>.+)_z\"\n",
    "\n",
    "num = 0\n",
    "# get all files from input_dir\n",
    "for subfolder in os.listdir(input_dir):\n",
    "    print(\"subfolder is:\", subfolder)\n",
    "    matcher = re.match(p, subfolder)\n",
    "    model_name = matcher.group(\"model_name\")\n",
    "    mode = matcher.group(\"mode\")\n",
    "    gamma = matcher.group(\"gamma\")\n",
    "    delta = matcher.group(\"delta\")\n",
    "    \n",
    "    bl_type = \"None\"\n",
    "    bl_type = (subfolder.split(\"_\")[-1]).split(\".\")[0]\n",
    "    \n",
    "    print(bl_type)\n",
    "    if bl_type != \"hard\":\n",
    "        if \"old\" in subfolder:\n",
    "            bl_type = \"soft\"\n",
    "        else:\n",
    "            bl_type = \"None\"\n",
    "        \n",
    "        \n",
    "    print(model_name, mode, gamma, delta, bl_type)\n",
    "    \n",
    "    z_score_path = os.path.join(input_dir, subfolder, \"z_score\")\n",
    "    if os.path.exists(z_score_path):\n",
    "        # print(\"subfolder is:\", subfolder)\n",
    "        files = os.listdir(z_score_path)\n",
    "        \n",
    "        for file in files:\n",
    "            # print(file)\n",
    "            # read jsons\n",
    "            matcher1 = re.match(p1, file)\n",
    "            if matcher1:\n",
    "                misson_name = matcher1.group(\"misson_name\")\n",
    "                threshold = 6.0\n",
    "            else:\n",
    "                threshold = file.split(\"_\")[-2]\n",
    "            \n",
    "            with open(os.path.join(z_score_path, file), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # get data\n",
    "            avarage_z = data[\"avarage_z\"]\n",
    "            sum = len(data[\"z_score_list\"])\n",
    "            # wm_pred_avarage = data[\"wm_pred_avarage\"]\n",
    "            \n",
    "            temp = pd.DataFrame({\n",
    "                \"model_name\": [model_name],\n",
    "                \"mode\": [mode],\n",
    "                \"mission_category\": [misson_name],\n",
    "                \"gamma\": [gamma],\n",
    "                \"delta\":[delta],\n",
    "                \"threshold\": [threshold],\n",
    "                \"bl_type\": [bl_type],\n",
    "                \"z_score\": [avarage_z],\n",
    "                \"sum\": [sum]})\n",
    "            \n",
    "            df = pd.concat([df, temp], ignore_index=True)\n",
    "            num += 1\n",
    "\n",
    "df.to_csv(\"z_score.csv\")           \n",
    "print(df)\n",
    "print(num)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            model_name             mission_name mode gamma delta  threshold  \\\n",
      "255  llama2-7b-chat-4k                      lcc   no   0.5   5.0        6.0   \n",
      "238  llama2-7b-chat-4k                 hotpotqa  old   0.9   5.0        6.0   \n",
      "242  llama2-7b-chat-4k  konwledge_understanding  old   0.9   5.0        6.0   \n",
      "218  llama2-7b-chat-4k  konwledge_understanding   v2   0.5   2.0        6.0   \n",
      "161  llama2-7b-chat-4k  konwledge_understanding  old   0.9   5.0        6.0   \n",
      "..                 ...                      ...  ...   ...   ...        ...   \n",
      "360  llama2-7b-chat-4k               multi_news  old   0.1  10.0        6.0   \n",
      "12   llama2-7b-chat-4k                    qmsum  gpt   0.1  10.0        6.0   \n",
      "276  llama2-7b-chat-4k               multi_news  old   0.1   5.0        6.0   \n",
      "315  llama2-7b-chat-4k               multi_news  old   0.1  10.0        6.0   \n",
      "15   llama2-7b-chat-4k               multi_news  gpt   0.1  10.0        6.0   \n",
      "\n",
      "    bl_type    z_score  sum  \n",
      "255    None  -0.915615  200  \n",
      "238    soft  -0.717057  200  \n",
      "242    soft  -0.534699  200  \n",
      "218    None  -0.507295  200  \n",
      "161    hard  -0.487937  200  \n",
      "..      ...        ...  ...  \n",
      "360    soft  63.581417  200  \n",
      "12     None  65.750604  200  \n",
      "276    hard  66.190720  200  \n",
      "315    hard  66.190720  200  \n",
      "15     None  66.471030  200  \n",
      "\n",
      "[363 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort_values(by=\"z_score\", ascending=True)\n",
    "df_sorted.to_csv(\"z_score_sorted.csv\")           \n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subfolder is: ref_old_soft_g0.1_d10.0\n",
      "old_soft_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_hard_g0.25_d5.0\n",
      "old_soft_g0.25_d15.0\n",
      "v2_g0.25_d15.0\n",
      "subfolder is: ref_old_hard_g0.25_d5.0\n",
      "old_hard_g0.25_d5.0\n",
      "old_soft_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_soft_g0.25_d15.0\n",
      "v2_g0.25_d15.0\n",
      "subfolder is: ref_old_soft_g0.25_d15.0\n",
      "old_soft_g0.25_d15.0\n",
      "old_soft_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_hard_g0.25_d5.0\n",
      "v2_g0.25_d15.0\n",
      "subfolder is: ref_gpt_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_soft_g0.1_d10.0\n",
      "old_hard_g0.25_d5.0\n",
      "old_soft_g0.25_d15.0\n",
      "v2_g0.25_d15.0\n",
      "subfolder is: ref_v2_g0.25_d15.0\n",
      "v2_g0.25_d15.0\n",
      "old_soft_g0.1_d10.0\n",
      "gpt_g0.1_d10.0\n",
      "old_hard_g0.25_d5.0\n",
      "old_soft_g0.25_d15.0\n",
      "            model_name             mission_name             ref_mode  \\\n",
      "0    llama2-7b-chat-4k               multi_news  old_soft_g0.1_d10.0   \n",
      "1    llama2-7b-chat-4k               alpacafarm  old_soft_g0.1_d10.0   \n",
      "2    llama2-7b-chat-4k                      lcc  old_soft_g0.1_d10.0   \n",
      "3    llama2-7b-chat-4k              longform_qa  old_soft_g0.1_d10.0   \n",
      "4    llama2-7b-chat-4k   konwledge_memorization  old_soft_g0.1_d10.0   \n",
      "..                 ...                      ...                  ...   \n",
      "175  llama2-7b-chat-4k   konwledge_memorization       v2_g0.25_d15.0   \n",
      "176  llama2-7b-chat-4k  konwledge_understanding       v2_g0.25_d15.0   \n",
      "177  llama2-7b-chat-4k                 hotpotqa       v2_g0.25_d15.0   \n",
      "178  llama2-7b-chat-4k               finance_qa       v2_g0.25_d15.0   \n",
      "179  llama2-7b-chat-4k                    qmsum       v2_g0.25_d15.0   \n",
      "\n",
      "                 det_mode threshold    z_score  sum  \n",
      "0          gpt_g0.1_d10.0       4.0  -2.051204  200  \n",
      "1          gpt_g0.1_d10.0       4.0  -1.545221  805  \n",
      "2          gpt_g0.1_d10.0       4.0  -0.608703  200  \n",
      "3          gpt_g0.1_d10.0       4.0  -1.884483  200  \n",
      "4          gpt_g0.1_d10.0       4.0   0.170509  200  \n",
      "..                    ...       ...        ...  ...  \n",
      "175  old_soft_g0.25_d15.0       4.0  -3.069688  200  \n",
      "176  old_soft_g0.25_d15.0       4.0  -1.915622  200  \n",
      "177  old_soft_g0.25_d15.0       4.0  -2.144303  200  \n",
      "178  old_soft_g0.25_d15.0       4.0  -9.799492  200  \n",
      "179  old_soft_g0.25_d15.0       4.0 -10.246890  200  \n",
      "\n",
      "[180 rows x 7 columns]\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "### process_mutual\n",
    "df = pd.DataFrame(columns=[\"model_name\", \"mission_name\", \"ref_mode\", \"det_mode\", \"threshold\", \"z_score\", \"sum\"])\n",
    "\n",
    "input_dir = \"./mutual_detect/llama2-7b-chat-4k\"\n",
    "\n",
    "num = 0\n",
    "p1 = r\"(?P<misson_name>[a-zA-Z_]+)_(?P<threshold>\\d+(\\.\\d+)?)_z\"\n",
    "\n",
    "# get all files from input_dir\n",
    "for subfolder in os.listdir(input_dir):\n",
    "    print(\"subfolder is:\", subfolder)\n",
    "    \n",
    "    ref_mode = subfolder.split(\"ref_\")[1]\n",
    "        \n",
    "    print(ref_mode)\n",
    "    \n",
    "    for subsubfolder in os.listdir(os.path.join(input_dir,subfolder)):\n",
    "        \n",
    "        det_mode = subsubfolder.split(\"_z\")[0]\n",
    "        print(det_mode)\n",
    "    \n",
    "        z_score_path = os.path.join(input_dir, subfolder, subsubfolder)\n",
    "        if os.path.exists(z_score_path):\n",
    "        # print(\"subfolder is:\", subfolder)\n",
    "            files = os.listdir(z_score_path)\n",
    "        \n",
    "            for file in files:\n",
    "                # print(file)\n",
    "                # read jsons\n",
    "                matcher1 = re.match(p1, file)\n",
    "                if matcher1:\n",
    "                    misson_name = matcher1.group(\"misson_name\")\n",
    "                    threshold = matcher1.group(\"threshold\")\n",
    "                with open(os.path.join(z_score_path, file), \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                # get data\n",
    "                avarage_z = data[\"avarage_z\"]\n",
    "                sum = len(data[\"z_score_list\"])\n",
    "                # wm_pred_avarage = data[\"wm_pred_avarage\"]\n",
    "                \n",
    "                temp = pd.DataFrame({\n",
    "                    \"model_name\": [model_name],\n",
    "                    \"ref_mode\": [ref_mode],\n",
    "                    \"mission_name\": [misson_name],\n",
    "                    \"det_mode\":[det_mode],\n",
    "                    \"threshold\": [threshold],\n",
    "                    \"z_score\": [avarage_z],\n",
    "                    \"sum\": [sum]})\n",
    "                \n",
    "                df = pd.concat([df, temp], ignore_index=True)\n",
    "                num += 1\n",
    "\n",
    "df.to_csv(\"mutual_z_score.csv\")           \n",
    "print(df)\n",
    "print(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size of chatglm2-6b-32k tokenizer is 64787\n",
      "The vocab size of chatglm2-6b-32k is 65024\n"
     ]
    }
   ],
   "source": [
    "# 导入transformers库\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "seed_everything(42)\n",
    "model2path = json.load(open(\"config/model2path.json\", \"r\"))\n",
    "dataset2maxlen = json.load(open(\"config/dataset2maxlen.json\", \"r\"))\n",
    "model_name = \"vicuna-13b\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 加载chatglm2-6b-32k模型的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model2path[model_name], trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model2path[model_name], trust_remote_code=True,\n",
    "                                                  output_scores=True, return_dict_in_generate=True, \n",
    "                                                  torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "# 获取tokenizer的词表大小\n",
    "vocab_size = model.config.padded_vocab_size\n",
    "vocab_size_tokenizer = len(tokenizer.get_vocab().values())\n",
    "\n",
    "# 打印词表大小\n",
    "print(\"The vocab size of chatglm2-6b-32k tokenizer is\", vocab_size_tokenizer)\n",
    "print(\"The vocab size of chatglm2-6b-32k is\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:18<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|User|>:Please give answer to the following question about knowledge. Note: If there are more than one answer, print them all and separate them with a semicolon (;). Please do not give anything other than the answers. Question:\\n\\nWhat is the given name of Alan B. Gaylor?<eoh>\\n<|Bot|>:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsq/anaconda3/envs/wmklmm/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 对数据进行分词\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m tokenizer(data, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 43\u001b[0m completions_text, completions_tokens  \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(completions_tokens)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(completions_text)\n",
      "File \u001b[0;32m/data2/tsq/WaterBench/generate.py:127\u001b[0m, in \u001b[0;36mGenerator.generate\u001b[0;34m(self, input_ids, max_new_tokens)\u001b[0m\n\u001b[1;32m    118\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    119\u001b[0m         input_ids, max_new_tokens\u001b[39m=\u001b[39mmax_new_tokens,\n\u001b[1;32m    120\u001b[0m         logits_processor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogit_processor_lst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         top_p\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mv2\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 127\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    128\u001b[0m         input_ids, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m    129\u001b[0m         logits_processor \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogit_processor_lst,\n\u001b[1;32m    130\u001b[0m         do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    131\u001b[0m         top_k\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    132\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampling_temp\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m \u001b[39m# remove the attached input from output for some model\u001b[39;00m\n\u001b[1;32m    136\u001b[0m scores \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mscores\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/transformers/generation/utils.py:1588\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1581\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1582\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1583\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1584\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1585\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1589\u001b[0m         input_ids,\n\u001b[1;32m   1590\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1591\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1592\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1593\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1594\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1595\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1596\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1597\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1598\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1599\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1600\u001b[0m     )\n\u001b[1;32m   1602\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1603\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/transformers/generation/utils.py:2642\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2639\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2641\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2642\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2643\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2644\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2645\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2646\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2647\u001b[0m )\n\u001b[1;32m   2649\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2650\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/internlm-chat-7b-8k/modeling_internlm.py:692\u001b[0m, in \u001b[0;36mInternLMForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    689\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    691\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    693\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    694\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    695\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    696\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    697\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    698\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    699\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    700\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    701\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    702\u001b[0m )\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/internlm-chat-7b-8k/modeling_internlm.py:580\u001b[0m, in \u001b[0;36mInternLMModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    572\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    573\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    574\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    581\u001b[0m         hidden_states,\n\u001b[1;32m    582\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    583\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    584\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    585\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    586\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    587\u001b[0m     )\n\u001b[1;32m    589\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/internlm-chat-7b-8k/modeling_internlm.py:291\u001b[0m, in \u001b[0;36mInternLMDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39m    past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 291\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states)\n\u001b[1;32m    293\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m    294\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    295\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    296\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    301\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/wmklmm/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/internlm-chat-7b-8k/modeling_internlm.py:92\u001b[0m, in \u001b[0;36mInternLMRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m [torch\u001b[39m.\u001b[39mfloat16, torch\u001b[39m.\u001b[39mbfloat16]:\n\u001b[1;32m     90\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> 92\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight \u001b[39m*\u001b[39;49m hidden_states\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "from generate import Generator\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "seed_everything(42)\n",
    "model_name = \"internlm-7b-8k\"\n",
    "model2path = json.load(open(\"config/model2path.json\", \"r\"))\n",
    "dataset2maxlen = json.load(open(\"config/dataset2maxlen.json\", \"r\"))\n",
    "dataset = \"alpacafarm\"\n",
    "max_gen = dataset2maxlen[dataset]\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model2path[model_name], trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model2path[model_name], device_map='auto', return_dict_in_generate=True, output_scores=True,trust_remote_code=True\n",
    "        ).to(device)\n",
    "\n",
    "watermark_args = Namespace(mode=\"v2\", gamma=0.25, delta=10.0, initial_seed=1234, dynamic_seed=None, bl_type=\"soft\", num_beams=1, sampling_temp=0.7, seeding_scheme=\"simple_1\", select_green_tokens=True)\n",
    "\n",
    "generator = Generator(watermark_args, tokenizer, model)\n",
    "with open(\"testfile.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data)\n",
    "\n",
    "# 对数据进行分词\n",
    "input = tokenizer(data, truncation=False, return_tensors=\"pt\").to(device)\n",
    "\n",
    "completions_text, completions_tokens  = generator.generate(input_ids=input.input_ids, max_new_tokens=max_gen)\n",
    "\n",
    "print(completions_tokens)\n",
    "print(completions_text)\n",
    "\n",
    "print(\"####################\")\n",
    "\n",
    "\n",
    "device_tmp = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model_tmp = AutoModelForCausalLM.from_pretrained(\n",
    "            model2path[model_name], device_map='auto', return_dict_in_generate=True, output_scores=True,trust_remote_code=True\n",
    "        ).to(device_tmp)\n",
    "generator_tmp = Generator(watermark_args, tokenizer, model_tmp).to(device_tmp)\n",
    "tokenizer_tmp = AutoTokenizer.from_pretrained(model2path[model_name], trust_remote_code=True).to(device_tmp)\n",
    "input_tmp = tokenizer_tmp(data, truncation=False, return_tensors=\"pt\").to(device_tmp)\n",
    "\n",
    "completions_text, completions_tokens  = generator_tmp.generate(input_ids=input_tmp.input_ids, max_new_tokens=max_gen)\n",
    "\n",
    "print(completions_tokens)\n",
    "print(completions_text)\n",
    "\n",
    "print(\"####################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_cpu:\n",
      "device: cpu is_cuda: False id: 140006862984896\n",
      "x_gpu:\n",
      "device: cuda:0 is_cuda: True id: 140000993795968\n"
     ]
    }
   ],
   "source": [
    "## Torch cuda\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========================== tensor to cuda\n",
    "# flag = 0\n",
    "flag = 1\n",
    "if flag:\n",
    "    x_cpu = torch.ones((3, 3))\n",
    "    print(\"x_cpu:\\ndevice: {} is_cuda: {} id: {}\".format(x_cpu.device, x_cpu.is_cuda, id(x_cpu)))\n",
    "\n",
    "    x_gpu = x_cpu.to(device)\n",
    "    print(\"x_gpu:\\ndevice: {} is_cuda: {} id: {}\".format(x_gpu.device, x_gpu.is_cuda, id(x_gpu)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "gpu_list=[0,1,3,4,5,6,7]\n",
    "gpu_list_str = ','.join(map(str, gpu_list))\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", gpu_list_str)\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "\n",
    "print(device_ids)\n",
    "\n",
    "device_ids.pop(0)\n",
    "if len(device_ids) == 0:\n",
    "    raise RuntimeError(\"No more available GPU devices\")\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('wmklmm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acdb7ce7ce850a195ea1a66ef30c55071e5db04eb4f79a2da398a13b17183832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
